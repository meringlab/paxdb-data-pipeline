from paxdb import spectral_counting as sc, scores
import os, shutil, pickle, sys, logging, re, sys, hashlib, yaml
import subprocess as sbp
from os.path import join
from datetime import date, datetime
from stringdb.repository import StringDbFileRepository

if len (sys.argv) == 3:
    step, rewrite = sys.argv[1:]
elif len(sys.argv) == 2:
    step = sys.argv[1]
    rewrite = False
else:
    step = 'all'
    rewrite = False

paxdb_version = 'v5.0'
FASTA_VER = '11.5'
root_dir = '/mnt/mnemo6/qingyao/paxDB/data-pipeline/'
input_dir = join(root_dir, 'input', paxdb_version)
output_dir = join(root_dir, 'output', paxdb_version)
if not os.path.isdir(output_dir):
    os.mkdir(output_dir)
data_dir = join(input_dir, 'datasets')
FASTA_DIR = join(root_dir, 'input', paxdb_version, 'fasta')
STRINGDB_REPO = join(root_dir, 'input', paxdb_version, 'stringdb')
interactions_format =  input_dir + '/interactions/{0}' + f'.network_v{FASTA_VER}_900.txt'
with open(f'paxdb/config.yml') as f:
    config = yaml.safe_load(f)
    


FORMAT = '%(asctime)s %(levelname)s %(message)s'
logging.basicConfig(format=FORMAT)
logger_file = logging.getLogger("write_log")
logger_file.addHandler(logging.FileHandler(f'../logs/{date.today().isoformat()}.log'))
logger_file.setLevel(30)
logger_file.propagate = False

logger_print = logging.getLogger("print_console")
logger_print.addHandler(logging.StreamHandler())
logger_print.setLevel(20)

def get_species(filepath):
    species =  filepath.split('/')[-1]
    if not re.fullmatch('\d+', species):
        # print(root)
        raise NameError("Did not find species")
    return species

### step null
picklefile = join(root_dir, 'output', paxdb_version, 'paxdb_datasets_info.pickle')

if not os.path.isfile(picklefile) or step == '0':
    logger_print.info('STEP 0')
    from PaxDbDatasetsInfo import PaxDbDatasetsInfo
    datasetsInfo = PaxDbDatasetsInfo()
    pickle.dump(datasetsInfo, open(picklefile, 'wb'))
else:
    datasetsInfo = pickle.load(open(picklefile, 'rb'))

### step 1 from spectral counts to abundance.
### input: .sc
### output: .abu
if step in ['1', 'all']:
    logger_print.info('STEP 1')

    for root,_,files in os.walk(data_dir):
        for f in files:
            if f.endswith('.sc'):
                try:
                    species = get_species(root)
                except NameError:
                    logger_file.error('.sc file: ' + f + ': no species found.')
                    continue
                # if f != 'Mus_musculus_SC_biomart_11364_E__2reps.sc':
                #     continue
                os.makedirs(join(output_dir,species), exist_ok = True)
                output_file = join(output_dir,species,f.replace('.sc','.abu'))
                
                if not rewrite and os.path.isfile(output_file):
                    continue
                sc.calculate_abundance_and_raw_spectral_counts(join(root, f),
                                                                output_file, 
                                                                species, 
                                                                FASTA_DIR, 
                                                                FASTA_VER)

### step 1.2 quality check copied original .abu files (no redundant IDs, in the string proteins)
### stringdb/ folder generated by running stringdb/repository.py
### just need to run once and check log.
### map species ID -> {'158878':'1280', '160490':'1314', '267671':'189518'}
if step == '1.2':
    paxdb_earlier_version = 'v4.1'
    paxdb_intermediate_version = 'v4.1.5'
    data_from_dir = join(root_dir, 'input', paxdb_earlier_version, 'datasets')
    data_to_dir =  join(root_dir, 'input', paxdb_intermediate_version, 'datasets')
    stringdb_dir = join(input_dir, 'stringdb')
    species_map = {'158878':'1280', '160490':'1314', '267671':'189518'}
    for root,_,files in os.walk(data_from_dir):
        try:
            species = get_species(root)
        except NameError:
            continue

        string_protein_set = set()
        with open(join(stringdb_dir, species+'-proteins.txt')) as fr:
            for l in fr:
                string_protein_set.add(l.split()[1].replace(species+'.',''))
        for f in files:
            if f.endswith('.abu'):
                file_to_check = join(data_to_dir, species, f)
                if not os.path.isfile(file_to_check):
                    file_to_check = join(data_to_dir, species_map[species], f)
                    if not os.path.isfile(file_to_check):
                        logger_file.error(species + '\t' + f + ' file not available!')
                        continue

                all_eids = set()
                dup_set = set()
                not_in_string_set = set()
                with open(file_to_check) as fr:
                    for i, l in enumerate(fr):
                        eid = l.split()[0]
                        if eid in all_eids:
                            dup_set.add(eid)
                        all_eids.add(eid)
                        if eid not in string_protein_set:
                            not_in_string_set.add(eid)

                logger_file.error(species + '\t' + f + f' duplicated count: {len(dup_set)} out of {i+1}')
                logger_file.error(species + '\t' + f + f' not in string count: {len(not_in_string_set)} out of {i+1}')
                logger_file.error(species + '\t' + f + f' duplicated: '+','.join(list(dup_set)))
                logger_file.error(species + '\t' + f + f' not in string: ' + ','.join(list(not_in_string_set)))

### step 2 map peptides from spectral count to string external ID with fasta file
### input: .sc
### output: .peptide

if step in ['2', 'all']:
    logger_print.info('STEP 2')
    for root,_,files in os.walk(data_dir):
        for f in files:
            if f.endswith('.sc'):
                    try:
                        species = get_species(root)
                    except NameError:
                        logger_file.error('.sc file: ' + f + ': no species found.')
                        continue 
                    # if species != '353153':
                    #     continue
                    output_file = join(output_dir,species,f.replace('.sc','.peptide'))
                    if not rewrite and os.path.isfile(output_file):
                        continue
                    sc.map_peptide(join(root, f),
                                    output_file,
                                    species, 
                                    FASTA_DIR, 
                                    FASTA_VER)

### step 3 compute IC score for datasets in output dir.
### input: .abu
### output: .zscores

if step in ['3', 'all']:
    logger_print.info('STEP 3')

    # output_dir = '/mnt/mnemo6/qingyao/paxDB/data-pipeline/test/proteomicsDB/output' # test
    for root,_,files in os.walk(output_dir):
        if 'score_downsampled' in root or 'mapping' in root:
                continue
        for f in files:
            if f.endswith('.abu'):
                try:
                    species = get_species(root)
                except NameError:
                    logger_file.error('.abu file: ' + f + ': no species found.')
                    continue
                # if species != '9606':
                #     continue
                interactions_file = interactions_format.format(species)
                abu_file = join(root, f)
                zscore_file = abu_file.replace('.abu','.zscores')
                if os.path.isfile(zscore_file):
                    continue
                scores.score_dataset(abu_file, zscore_file, interactions_file)
                
### step 4 downsample datasets to prepare for integration (1 min)
if step in ['4', 'all']:
    logger_print.info('STEP 4')
    from scores_shared_protein_set import compute_shared_proteins
    from scores_shared_protein_set import enumerate_dataset_files
        
    def downsample_dataset(input_file, num_abundances, proteins_counts, max_coverage, output_file):
        if os.path.exists(output_file):
            return
        if num_abundances <= max_coverage:
            shutil.copy(input_file, output_file)
            return
        abundances = dict()
        with open(input_file) as abu:
            for line in abu:
                r = line.strip().split('\t')
                abundances[r[0]] = r[1]

        most_frequent_protein_ids = sorted(proteins_counts, key=proteins_counts.get, reverse=True)
        most_frequent_protein_ids = [p for p in most_frequent_protein_ids if p in abundances]

        with open(output_file,'w') as abu:
            num_added = 0
            for protein in most_frequent_protein_ids:
                abu.write('{0}\t{1}\n'.format(protein, abundances[protein]))
                if num_added == max_coverage:
                    break
                num_added += 1

    def get_organ_for(datasetfile):
        dataset_name = os.path.basename(datasetfile)
        if dataset_name.endswith('.integrated'):
            m = re.match(r"\d+\-(.+)-integrated.integrated", dataset_name)
            if m:
                return m.groups()[0]
            raise ValueError('failed to get organ for {0}'.format(datasetfile))
        
        info = datasetsInfo.get_dataset_info(species, os.path.splitext(dataset_name)[0])
        return info.organ


    def sort_abundances(dataset):
        cmd="cat '{0}'".format(dataset)
        cmd= cmd + "| awk '{print $2,$1}' | sort -gr  | awk '{print $2,$1}'"
        sorted_out = sbp.check_output(cmd, shell=True).decode('utf8')
        sorted_abundances = [l +'\n' for l in sorted_out.split('\n')]
        return sorted_abundances

    dataset_f_stems = set()
    for species, info in datasetsInfo.datasets.items():
        for organ, datasets in info.items():
            for dataset in datasets:
                dataset_f_stems.add(os.path.splitext(dataset.dataset)[0])
    
    for root,_,files in os.walk(output_dir):
        if 'score_downsampled' in root or 'mapping' in root:
            continue

        for f in files:
            
            if f.endswith('.abu'):
                f_stem = os.path.splitext(f)[0]
                if f_stem not in dataset_f_stems:
                    
                    continue
                    
                try:
                    species = get_species(root)
                except NameError:
                    logger_file.error('.abu file: ' + f + ': no species found.')
                    continue
                
                organ = get_organ_for(f)
                # print(f)
                # print(species, organ, root)
                output_path = join(output_dir, 'score_downsampled', species)

                if not os.path.isdir(output_path):
                    os.makedirs(output_path, exist_ok = True)
                output_file = join(output_path, f)
                # print(output_file)
                if not rewrite and os.path.isfile(output_file):
                    continue

                datasets = enumerate_dataset_files(species, organ, root)
                if len(datasets) < 2:
                    continue

                total = datasetsInfo.datasets[species][organ][0].genome_size

                (proteins_counts,num_abundances) = compute_shared_proteins(datasets)
                # print(proteins_counts,num_abundances)
                max_coverage = max(min(num_abundances.values()), int(total * 0.3))
                if f.endswith('.integrated'):
                    num_input_abundances = sum([1 for line in open(join(root,f))])
                    print(num_input_abundances, len(sort_abundances(join(root,f))))
                else:
                    num_input_abundances = num_abundances[join(root,f)]

            
                downsample_dataset(join(root, f), num_input_abundances, proteins_counts, max_coverage, output_file)



# ## step 5 integrate datasets by species and organ (July 14 15:09 - July 16 08:47:19)
if step in ['5', 'all']:
    logger_print.info('STEP 5')
    for root, _, files in os.walk(join(output_dir,'score_downsampled')):
        for f in files:
            if f.endswith('.abu'):
                species = get_species(root)
                interactions_file = interactions_format.format(species)
                abu_file = join(root, f)
                zscore_file = abu_file.replace('.abu','.zscores')
                if os.path.isfile(zscore_file):
                    continue
                scores.score_dataset(abu_file, zscore_file, interactions_file)

    from DatasetIntegrator import DatasetIntegrator, RScriptRunner
    sorter = scores.DatasetSorter()
    for species in datasetsInfo.datasets:
        # if species not in ['353153','1314']:
        #     continue

        for organ in datasetsInfo.datasets[species]:
            if len(datasetsInfo.datasets[species][organ]) < 2:
                continue
            try:
                sorted_datasets = sorter.sort_datasets(join(output_dir,'score_downsampled', species))
                
            except:
                logger_print.error("failed to sort datasets for {0}: {1}".format(species, sys.exc_info()[1]))
                logger_file.error("failed to sort datasets for {0}: {1}".format(species, sys.exc_info()[1]))
                continue
            
            parameters = []
            # sort by scores:
            by_organ = [os.path.splitext(d.dataset)[0] for d in datasetsInfo.datasets[species][organ] if
                        not d.integrated]
            input_list = \
                [[join(output_dir, species, d + '.abu') for d in sorted_datasets if d in by_organ],
                    # dependency: zscores affect dataset integration order:
                    [join(output_dir, 'score_downsampled',species, d + '.zscores') for d in sorted_datasets if d in by_organ]
                ]
            # print(input_list)
            output_file = join(output_dir, species, "{0}-{1}-integrated.integrated".format(species, organ))
            
            if not rewrite and os.path.isfile(output_file):
                continue

            ## compute
            input_files = input_list[0]
            logger_print.info('integrating {0}-{1} into {2}'.format(species, organ, output_file))
            interactions_file = interactions_format.format(species)
            
            out_dir = join(output_dir, species)
            print('process', len(input_files), 'datasets...')
            
            rscript = RScriptRunner('integrate.R', [out_dir])
            integrator = DatasetIntegrator(output_file, input_files, rscript) # organ != 'WHOLE_ORGANISM')
            weights = integrator.integrate(interactions_file)
            logger_print.info(species + ' weights: ' + ','.join([str(w) for w in weights]))
            
## step 6 score integrated datasets (July 18 16:27 - Jul 18 23:04)
if step in ['6', 'all']:
    logger_print.info('STEP 6')
    for root, _, files in os.walk(output_dir):
        for f in files:
            if f.endswith('.integrated'): 
                try:
                    species = get_species(root)
                except NameError:
                    continue
                interactions_file = interactions_format.format(species)
                abu_file = join(root, f)
                zscore_file = abu_file.replace('.integrated','.zscores')
                if not rewrite and os.path.isfile(zscore_file):
                    continue
                scores.score_dataset(abu_file, zscore_file, interactions_file)


## step 7 rank by abundances and use string_internal_id as first column
### input: .abu or .integrated 
### output: .pax

if step in ['7', 'all']:
    logger_print.info('STEP 7')

    def load_stringdb_id_mapper(species):    
        repo = StringDbFileRepository(STRINGDB_REPO)
        proteins = repo.load_proteins(species)
        mapper = {}
        for p in proteins:
            mapper[p.externalId] = p.id
        return mapper

    count = 0
    for root,_,files in os.walk(output_dir):
        if 'score_downsampled' in root or 'mapping' in root:
                continue
        for f in files:
            if f.endswith('.abu') or f.endswith('.integrated'):
                print('currently processing '+ root + '/' + f +'...')
                try:
                    species = get_species(root)
                except NameError:
                    continue
                id_mapper = load_stringdb_id_mapper(species)
                output_file = join(root, os.path.splitext(f)[0]+'.pax')

                if not rewrite and os.path.isfile(output_file):
                    continue

                ## index by second column value
                abu_line = []
                with open(join(root,f)) as f_read:
                    for l in f_read:
                        p_id = l.split('\t')[0]
                        p_rest = l.split('\t')[1:]
                        try:
                            abu_line.append([float(p_rest[0]), p_id, p_rest])
                        except ValueError:
                            print('abnormal line, skip file... ')
                abu_line.sort(key = lambda x: x[0], reverse=True)

                with open(output_file, 'w') as f_write:
                    for l_items in abu_line:
                        _, p_id, p_rest = l_items
                        try:
                            internal_id = id_mapper[p_id]
                        except KeyError:
                            logger_print.debug(species + ': ' + f + ' protein: ' + p_id)
                            logger_file.error(species + ': ' + f + ' protein: ' + p_id)
                            continue
                        else:
                            f_write.write('\t'.join([str(internal_id), p_id, '\t'.join(p_rest)]))

                count+=1

    logger_print.info(f'processed {count} files into .pax format')

## step 7.1 round to 2 digits
### input: .pax
### output: .pax_rounded

    logger_print.info('STEP 7.1')
    def round_abundance(value): 
        ## inherited from previous version
        if type(value) == str:
            value = float(value)
        if value >= 100:
            return round(value)
        elif value >= 10:
            return round(value, 1)
        elif value >= 1:
            return round(value, 2)
        elif value >= 0.001:
            return round(value, 3)
        return 0

    for root,_,files in os.walk(output_dir):
        if 'score_downsampled' in root or 'mapping' in root:
                continue
        for f in files:
            if f.endswith('.pax'):
                output_file = join(root, os.path.splitext(f)[0] + '.pax_rounded')
                if not rewrite and os.path.isfile(output_file):
                    continue
                with open(join(root, f)) as src:
                    with open(output_file, 'w') as dst:
                        print(output_file)
                        for line in src:
                            rec = line.split('\t')
                            rec[2] = (round_abundance(rec[2]))
                            newline = '\t'.join([str(r) for r in rec])
                            dst.write(newline)
                            if not newline.endswith('\n'):
                                dst.write('\n')


## step 8 generate final output files
if step in ['8', 'all']:

    logger_dataset = logging.getLogger("write_log_dataset")
    fh = logging.FileHandler(f'../logs/{date.today().isoformat()}_dataset.log')
    fh.setLevel(10)
    fh.setFormatter(logging.Formatter('%(message)s'))
    logger_dataset.addHandler(fh)

    logger_print.info('STEP 8')
    def make_name_readable(info):
        
        filename = info.dataset
        species_name = info.species_name
        tissue = info.organ
        if hasattr(info,'publication'): 
            publication = info.publication
        else:
            publication = ''
        if hasattr(info, 'quantification_method'):
            method = info.quantification_method
        else:
            method = 'N'

        if hasattr(info, 'condition_media'):
            condition = info.condition_media
        else:
            condition = 'N'
            
        if tissue == "cellline":
            tissue = "Cell line"

        tissue = " ".join(tissue.split("_"))
        tissue = tissue.lower()
        tissue = tissue.capitalize()

        name = tissue

        if condition and condition != "N":
            condition = " ".join(condition.split("_"))
            name += ", " + condition.capitalize()

        if method and method != "N":
            method = " ".join(method.split("_"))
            if "pectral counting" in method: method = "SC"
            if method != "MAPPED BY AUTHORS":
                name += ", " + method

        if not info.integrated:
            publication = publication.capitalize()
            publication = publication.replace(" ", "")
            name += " (" + publication + ")"
        else:
            name = filename
            name = name.split("-", 1)[-1].lower().capitalize()
            name = name.split("-")[0]
            name = name.split(",")[0] + " (Integrated)"
            name = " ".join(name.split("_"))

        name = name.replace("Spectral counting", "SC")
        name = species_name + " - " + name
        return name

    def write_dataset_title(dst, info, dataset_score='1', dataset_weight='100', coverage='54%'):
        if not info.integrated:
            string2 = "#score: " + dataset_score + "\n" + "#weight: " + dataset_weight + "%\n"
            if info.condition_media:
                string3 = "#description: abundance based on " + info.quantification_method + ", " + info.condition_media + ", Interaction consistency score: " + dataset_score + ", Coverage: " + coverage + "\n"
            else:
                string3 = "#description: abundance based on " + info.quantification_method + ", " + "Interaction consistency score: " + dataset_score + ", Coverage: " + coverage + "\n"
            string4 = "#organ: " + info.organ + "\n" + "#integrated: false\n#coverage: " + coverage + '\n' + "#link: " + info.source_link + '\n'
        else:
            string2 = "#score: " + dataset_score + "\n"
            string3 = "#description: integrated dataset: weighted average of all " + info.species_name + ' ' + info.organ + ", Interaction consistency score: " + \
                    dataset_score + ", Coverage: " + coverage + "\n"
            string4 = "#organ: " + info.organ + "\n#integrated: true\n#coverage: " + coverage + '\n'


        name = make_name_readable(info)
        filename = os.path.splitext(info.dataset)[0]
        taxon = str(info.species_id)
        if not filename.startswith(taxon):
            filename = taxon + '-' + filename
        elif not filename.startswith(taxon + '-'):
            filename = filename.replace(taxon, taxon + '-')
            filename = filename.replace('-_', '-')
        dataset_id = int.from_bytes(hashlib.sha256((paxdb_version+'-'+filename).encode('utf-8')).digest()[:4], 'little')

        dst.write('#id: %d\n' % dataset_id)
        dst.write("#name: {0}\n".format(name))
        dst.write(string2)
        dst.write(string3)
        dst.write(string4)
        dst.write("#publication_year: ")

        if info.integrated:
            dst.write(str(date.today().year))
        elif hasattr(info,'publication'):
            m = re.match(r".+,\s*([0-9]{4})", info.publication)
            if m:
                yr = m.groups()[0]
                try:
                    year = int(yr)
                    if year > 1990 and year <= date.today().year:
                        dst.write(yr)
                except:
                    logger_print.error("failed to get parse year for {0}".format(info.dataset))
                    logger_file.error("failed to get parse year for {0}".format(info.dataset))
        dst.write('\n')
        
        dst.write("#filename: " + filename + ".txt\n")
        dst.write("#\n#internal_id\tstring_external_id\tabundance")
        if hasattr(info, 'quantification_method'):
            if info.quantification_method and info.quantification_method.lower().startswith("spectral counting"):
                dst.write("\traw_spectral_count")
        dst.write('\n')

    def get_dataset_weight(filepath, info):
        dataset_name = os.path.splitext(os.path.basename(filepath))[0]
        if 'integrated' in filepath: # this file is integrated over more than 1 sample
            return ""
        else:
            dataset_name = os.path.splitext(os.path.basename(filepath))[0]
            print(dataset_name)
            integrated_file = join(os.path.dirname(filepath), f'{info.species_id}-{info.organ}-integrated.weights')
            if not os.path.isfile(integrated_file): # it is only file for the species-organ
                return "100"
            with open(integrated_file) as f:
                for l in f:
                    if l.startswith(dataset_name):
                        return l.strip().split(":")[1]
            logger_print.debug(f"No weights found for {dataset_name} in {info.species_id}-{info.organ}")
            logger_file.debug(f"No weights found for {dataset_name} in {info.species_id}-{info.organ}")
            return ""

    # current_id =  datasetsInfo.get_dataset_number()['integrated']
    
    for root,_,files in os.walk(output_dir):
        if 'score_downsampled' in root or 'mapping' in root:
                continue
        for f in files:
            if f.endswith('.pax_rounded'):
                
                try:
                    species = get_species(root)
                except NameError:
                    continue
                input_file = join(root, f)
                f_stem = os.path.splitext(f)[0]
                output_file = join(root, f_stem + '.txt')
                if not rewrite and os.path.isfile(output_file):
                    continue
                
                # integrated file
                if f_stem.endswith('integrated'):
                    print(f)
                    organ = os.path.splitext(os.path.basename(f).split('-')[1])[0]
                    i = next(iter(datasetsInfo.datasets[species].values()))[0]  # any other dataset info
                    info = type('DatasetInfo', (object,),
                                {'dataset': os.path.basename(output_file), 'integrated': True, 'organ': organ,
                                    'publication': None, 'condition_media': None,'quantification_method':None,
                                    'species_id': species,
                                    'genome_size': i.genome_size,
                                    'species_name': i.species_name #, 'dataset_id': current_id
                                    })()
                    
                else:
                    try:
                        info = datasetsInfo.get_dataset_info(species, f_stem)
                    except ValueError: ## not a dataset
                        logger_dataset.info(root + f + 'dataset name not in datasetsInfo.') 
                        continue
                dataset_score = open(join(root, os.path.splitext(f)[0] + '.zscores')).readline().strip()
                dataset_weight = get_dataset_weight(input_file, info)
                num_proteins = 0
                with open(input_file) as src:
                    num_proteins = sum(1 for line in src)
                        
                coverage = str(round(100 * num_proteins / info.genome_size))

                with open(output_file, 'w') as dst:
                    write_dataset_title(dst, info, dataset_score, dataset_weight, coverage)
                    with open(input_file) as src:
                        for line in src:
                            dst.write(line)

## step 9 copy final output files into final/ folder
## copy renamed files to final/
if step in ['9', 'all']:
    logger_print.info('STEP 9')
    final_dir = join(output_dir,'final')
    if not os.path.isdir(final_dir):
        os.makedirs(final_dir)
    for root,_,files in os.walk(output_dir):
        if 'final' in root:
            continue
        for f in files:
            if f.endswith('.txt'):
                try:
                    species = get_species(root)
                except NameError:
                    logger_file.error('.txt file: ' + f + ': no species found.')
                    continue
                if f.startswith(species):
                    output_file = join(final_dir,f.replace(species+'_', species+'-'))
                else:
                    output_file = join(final_dir,species+'-'+f)
                shutil.copyfile(join(root,f), output_file)